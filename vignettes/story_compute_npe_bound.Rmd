---
title: "Computing Bounds Under Non-Constant Treatment Effect"
output:
  rmarkdown::html_document:
    toc: true
    toc_float: true
    toc_depth: 2
    number_sections: true
    highlight: "textmate"
    css: "custom.css"
bibliography: "ggsd.bib"
vignette: >
 %\VignetteIndexEntry{Computing Bounds Under Non-Constant Treatment Effect}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Overview

We consider group sequential designs with possibly non-constant treatment effects over time.
This can be useful for situations such as an assumed non-proportional hazards model as laid out in `vignette("NPEbackground", package="gsdmvn")`. 
In general, we assume $K\ge 1$ analyses with statistical information $\mathcal{I}_k$ and information fraction $t_k=\mathcal{I}_k/\mathcal{I}_k$ at analysis $k$, $1\le k\le K$.
We denote the null hypothesis $H_{0}$: $\theta(t)=0$ and an alternate hypothesis $H_1$: $\theta(t)=\theta_1(t)$ for $t> 0$ where $t$ represents the information fraction for a study.
While a study is planned to stop at information fraction $t=1$, we define $\theta(t)$ for $t>0$ since a trial can overrun its planned statistical information at the final analysis. 
As before, we use a shorthand notation in to have $\theta$ represent $\theta()$, $\theta=0$ to represent 
$\theta(t)\equiv 0$ for all $t$ and $\theta_1$ to represent $\theta_i(t_k)$, the effect size at analysis $k$, $1\le k\le K$.

For our purposes, $H_0$ will represent no treatment difference, but it could represent a non-inferiority hypothesis.
Recall that we assume $K$ analyses and bounds $-\infty \le a_k< b_k<\le \infty$ for $1\le k < K$ and $-\infty \le a_K\le b_K<\infty$. 
We denote the probability of crossing the upper boundary at analysis $k$ without previously crossing a bound by

$$\alpha_{k}(\theta)=P_{\theta}(\{Z_{k}\geq b_{k}\}\cap_{j=1}^{k-1}\{a_{j}\le Z_{j}< b_{j}\}),$$
$k=1,2,\ldots,K.$
The total probability of crossing an upper bound prior to crossing a lower bound is denoted by 

$$\alpha(\theta)\equiv\sum_{k=1}^K\alpha_k(\theta).$$

For non-binding bounds, we define the probability 

$$\alpha_{k}^{+}(\theta)=P_{\theta}\{\{Z_{k}\geq b_{k}\}\cap_{j=1}^{k-1} \{Z_{j}< b_{j}\}\}$$
which ignores the lower bounds when computing upper boundary crossing probabilities.
The non-binding Type I error is the probability of ever crossing the upper bound
when $\theta=0$. The value $\alpha^+_{k}(0)$ is commonly referred to as
the amount of Type I error spent at analysis $k$, $1\leq k\leq K$. The
total upper boundary crossing probability for a trial is denoted in this
one-sided scenario by
$$\alpha^+(\theta) \equiv\sum_{k=1}^{K}\alpha^+_{k}(\theta).$$
We will primarily be interested in $\alpha(\theta)$ to compute power when $\theta > 0$.
For Type I error, we may be interested in $\alpha(0)$ for binding lower bounds, but more often we will consider non-binding Type I error calculations, $\alpha^{+}(0)$.

We denote the probability of crossing a lower bound at analysis $k$ without previously crossing any bound by

$$\beta_{k}(\theta)=P_{\theta}((Z_{k}< a_{k}\}\cap_{j=1}^{k-1}\{ a_{j}\le Z_{j}< b_{j}\}).$$

Efficacy bounds $b_k$, $1\le k\le K$, for a group sequential design will be derived to control Type I at some level $\alpha=\alpha(0)$.

Lower bounds $a_k$, $1\le k\le K$ may be used to control boundary crossing probabilities under either the null hypothesis (2-sided testing), the alternate hypothesis or some other hypothesis (futility testing).

Thus, we may consider up to 3 values of $\theta(t)$: 

- under the null hypothesis $\theta_0(t)=0$ for computing efficacy bounds, 
- under a value $\theta_1(t)$ for computing lower bounds, and
- under a value $\theta_a(t)$ for computing sample size or power.

We refer to the information under these 3 assumptions as $\mathcal{I}^{(0)}(t)$, $\mathcal{I}^{(1)}(t)$, and $\mathcal{I}^{(a)}(t)$, respectively. Often we will assume
$\mathcal{I}(t)=\mathcal{I}^{(0)}(t)=\mathcal{I}^{(1)}(t)=\mathcal{I}^{(a)}(t).$

We note that information may differ under different values of $\theta(t)$. 
For fixed designs, \cite{LachinBook} computes sample size based on different variances under the null and alternate hypothesis.

## Spending bounds

We consider different boundary types in the **gsDesign** package and simplify them into two types according to whether lower bounds are binding or non-binding.
The concept is to implicitly derive Z-value bounds $a_k, b_k, k=1,\cdots,K$ based on probabilities specified in the following table. 
We include the `test.type` argument from the `gsDesign::gsDesign()` function for reference.


   `test.type`      Upper bound         Lower bound        Design type
  ------------- ------------------- -------------------  ------------------------------------------
        1        $\alpha_k^{+}(0)$       None            One-sided efficacy
        2        $\alpha_k(0)$      $\alpha_k(0)$        2-sided symmetric
        3        $\alpha_k(0)$      $\beta_k(\theta_a)$  $\beta$-spending with binding futility
        4        $\alpha_k^{+}(0)$  $\beta_k(\theta_a)$  $\beta$-spending with non-binding futility
        5        $\alpha_k(0)$      $\beta_i(\theta_1)$  $\theta$-spending with binding futility   
        6        $\alpha^{+}(0)$    $\beta_i(\theta_1)$  $\theta$-spending with non-binding futility

  : Boundary crossing probabilities used to set Z-value boundaries

This can be reduced to just two types distinguishing by whether or not lower bounds are binding or non-binding:

   `test.type`      Upper bound         Lower bound        Design type
  ------------- ------------------- -------------------  ------------------------------------------
    2, 3, 5     $\alpha_k(0)$       $\beta_k(\theta)$    Binding lower bound
    1, 4, 6     $\alpha_k^{+}(0)$   $\beta_k(\theta)$    Non-binding lower bound

  : Reduced options for boundary crossing probabilities used to set Z-value boundaries


In this second table we have used $\theta=0$ to derive the upper bound to control Type I error in all cases.
We have chosen some arbitrary $\theta$ which could be 0 for any other `test.type`, $\theta_a$ for $\beta$-spending or some arbitrary $\theta_1$ otherwise.
We note that for a one-sided design we let $\beta_k(\theta)=0$ so that $a_k=-\infty, k=1,\cdots,K$.
For `test.type=3, 4` we let $\theta=\theta_a$, while for `test.type=5, 6` $\theta\ge 0$ is arbitrary.
We note that asymmetric $\alpha$-spending bounds can be derived using `test.type > 2` and $\theta=0.$

## Two-sided testing and design

We denote an alternative $H_{a}$: $\theta(t)=\theta_a(t)$; we will always assume $H_a$ for power calculations; when using $\beta$-spending we will also use $H_a$ for controlling lower boundary $a_k$ crossing probabilities by letting $\theta=\theta_a$ for lower bound spending. 
A value of $\theta(t)>0$ will reflect a positive benefit. 
We will not restrict the alternate hypothesis to $\theta_a(t)>0$ for all $t$.
The value of $\theta(t)$ will be referred to as the (standardized) treatment effect at information fraction $t$.

We assume there is interest in stopping early if there is good evidence to reject one hypothesis in favor of the other. 

If $a_k= -\infty$ at analysis $k$ for some $1\le k\le K$ then the alternate hypothesis cannot be rejected at analysis $k$; i.e., there is no futility bound at analysis $k$. 
For $k=1,2,\ldots,K$, the trial is stopped at analysis $k$ to reject $H_0$ if $a_j<Z_j< b_j$, $j=1,2,\dots,i-1$ and $Z_k\geq b_k$. 
If the trial continues until stage $k$ without crossing a bound and $Z_k\leq a_k$ then $H_1$ is rejected in favor of $H_0$, $k=1,2,\ldots,K$. 
Note that if $a_K< b_K$ there is the possibility of completing the trial without rejecting $H_0$ or $H_1$ unless $a_K=b_K.$ 

### Haybittle-Peto and spending bounds

The recursive algorithm of the previous section allows computation of both spending bounds and Haybittle-Peto bounds.
For a Haybittle-Peto efficacy bound, one would normally set $b_k=\Phi^{-1}(1-\epsilon)$ for $k=1,2,\ldots,K-1$ and some small $\epsilon>0$ such as $\epsilon= 0.001$ which yields $b_k=3.09$.
While the original proposal was to use $b_K=\Phi^{-1}(1-\alpha)$ at the final analysis, to fully control one-sided Type I error at level $\alpha$ we suggest computing the final bound $b_K$ using the above algorithm so that $\alpha(0)=\alpha$.

Bounds computed with spending $\alpha_k(0)$ at analysis $k$ can be computed by using equation (9) for $b_1$.
Then for $k=2,\ldots,K$ the algorithm of the previous section is used.
As noted by @JTBook, $b_1,\ldots,b_K$ if determined under the null hypothesis depend only on $t_k$ and $\alpha_k(0)$ with no dependence on $\mathcal{I}_k$, $k=1,\ldots,K$.
When computing bounds based on $\beta_k(\theta)$, $k=1,\ldots,K$,  where some $\theta(t_k)\neq 0$ we have an additional dependency with $a_k$ depending not only on $t_k$ and $b_k$, $k=1,\ldots,K$, but also on the final total information $\mathcal{I}_K$.
Thus, a spending bound under something other than the null hypothesis needs to be recomputed each time $\mathcal{I}_K$ changes, whereas it only needs to be computed once when $\theta(t_k)=0$, $k=1,\ldots,K$.

### Bounds based on boundary families

Assume constants $b_1^*,\ldots,b_K^*$ and a total targeted one-sided Type I error $\alpha$.
We wish to find $C_u$ as a function of $t_1,\ldots t_K$ such that if $b_k=C_ub_k^*$ then $\alpha(0)=\alpha.$
Thus, the problem is to solve for $C_u$. If $a_k$, $k=1,2,\ldots,K$ are fixed then this is a simple root finding problem.
Since one normally normally uses non-binding efficacy bounds, it will normally be the case that $a_k=-\infty$, $k=1,\ldots,K$ for this problem.

Now we assume constants $a_k^*$ and wish to find $C_l$ such that if $a_k=C_la_k^*+\theta(t_k)\sqrt{\mathcal{I}_k}$ for $k=1,\ldots,K$ then
$\beta(\theta)=\beta$. If we use the constant upper bounds from the previous paragraph, finding $C_l$ is a simple root-finding problem.

For 2-sided symmetric bounds with $a_k=-b_k$, $k=1,\ldots,K$, we only need to solve for $C_u$ and again use simple root finding. 

At this point, we do not solve for this type of bound for asymmetric upper and lower bounds.

## Sample size

For sample size, we assume $t_k$, and $\theta(t_k)$ $1,\ldots,K$ are fixed. 
We assume $\beta(\theta)$ is decreasing as $\mathcal{I}$ is decreasing.
This will automatically be the case when $\theta(t_k)>0$, $k=1,\ldots,K$ and for many other cases.
Thus, the information required is done by a search for $\mathcal{I_K}$ that yields $\alpha(\theta)$ yields the targeted power.

## References
